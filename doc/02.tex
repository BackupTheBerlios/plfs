\chapter{Arquitectura del sistema}

\textbf{\textit{En este capítulo comentamos la arquitectura general del
sistema, los componentes necesarios, el sistema de ficheros y las operaciones
que debe implementar.}}



\section{Componentes}

En este apartado mostraremos un esquema inicial de los principales
componentes que necesitaría el sistema presentado (figura
\ref{fig:arquitectura}):

\begin{description}
	\item [\kplfs:] PlanetLab File System (Kernel-mode Side)\\
		Componente del sistema de ficheros para PlanetLab que funciona dentro
		del núcleo de Linux.
		\\
		Este componente, puede en realidad ser varios, cada uno dando
		unos servicios concretos (uno para el desplegamiento, uno para
		monitorización, \ldots).

	\item [\uplfs:] PlanetLab File System (User-mode Side)\\
		Componente del sistema de ficheros para PlanetLab que funciona en modo
		usuario.
		\\
		Este componente en realidad hace de intermediario entre el
		propio sistema de ficheros (es decir, el núcleo) y los
		componentes que permiten realizar las funcionalidades y
		comunicaciones (en nuestro caso, sería \dpld).

	\item [\dpld:] Deployer Daemon\\
		Componente encargado de mandar las órdenes de desplegamiento y
		mantener actualizada la visión que tiene el usuario sobre los
		slices de desplegamiento.

	\item [\pldb :] PlanetLab Data Base (para obtener inicialmente la lista
		de nodos de un slice)\\
		Componente para obtener datos de PlanetLab Central (PLC).

	\item [\umcc :] User-mode Multicast Client\\
		Componente encargado de las comunicaciones entre un sistema cualquiera
		y la red multicast.

	\item [\umcr :] User-mode Multicast Router\\
		Componente encargado de enrutar paquetes multicast.

	\item [\dplc :] Deployer Client\\
		Componente encargado de desplegar los ficheros en un nodo e informar
		de los slices que tiene creados.

	\item [\famon:] File Alteration Monitor\\
		Componente encargado de vigilar qué ficheros cambian en un nodo.
\end{description}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{arquitectura.eps}
	\caption{Arquitectura del Sistema}
	\label{fig:arquitectura}
\end{figure}



\section{Lógica del sistema}

El sistema, además de las funcionalidades propias de despliegue de
aplicaciones, debe poder realizar las siguientes operaciones para poder saber
como llegar y a dónde puede llegar. Es decir, debe poder conseguir la
información necesaria sobre los slices disponibles en PlanetLab y sus nodos, y
por otra parte, debe poder descubrir su punto de entrada a la red multicast
desde el exterior.

De modo que el sistema debe permitir:

\begin{itemize}
	\item Descubrimiento de slices en PlanetLab \\
		Al acceder al sistema de ficheros (\kplfs-\uplfs), hay que preguntar a
		un servicio de PlanetLab si existe o no un slice dado (al hacer un
		move de un ejectuable a un directorio que es un slice) o cuáles
		existen (al hacer un ls para ver los slices disponibles).

		Para ello, hace falta encontrar el nodo con el servicio de PlanetLab
		de BD \textbf{más cercano} (por ahora solo hay uno, y es
		\textit{PlanetLab Central}) y preguntarle:
		\begin{itemize}
			\item si existe un slice concreto

			\item qué slices hay
		\end{itemize}

	\item Descubrimiento de nodos en PlanetLab \\
		Como en el caso anterior, nos hace falta preguntar a PlanetLab por los
		nodos, tanto a nivel general, como por los asociados a un slice.

	\item Descubrimiento del nodo de entrada a la red de distribución
		multicast.
\end{itemize}

Con tal de minimizar la latencia y ocupación innecesaria del ancho de banda,
para todos los anteriores puntos hace falta encontrar el nodo \textbf{más
cercano} que da un servicio concreto.

Para ello, se podría utilizar un servicio DNS al estilo Akamai, tal como
utiliza google, o algo como CoDNS \cite{CoDNS}, subproyecto de Codeen
\cite{Codeen}, que nos dan la IP del nodo más cercano que proporciona un
servicio conreto.



\section{Esquema de \plfs (PlanetLab File System)}

Antes de comentar el esquema del árbol de ficheros y directorios de \plfs, es
necesario dejar bien claro que hemos diseñado el sistema con la idea de
permitir una cierta flexibilidad al usuario y que pueda diferenciar entre dos
tipos de ficheros:

\begin{description}
	\item [Los ficheros que se despliegan como shared]:\\
		Que todos los nodos del slice destino tendrán por igual, bajo la
		condición que además deben de ser inmutables (ej: el programa
		principal de la aplicación).

	\item [Los ficheros que se despliegan como unshared]:\\
		Que serán propios de cada nodo (ej: cada nodo podría asumir un
		rol distinto si tuviera ficheros de configuración propios).
\end{description}

Como veremos ahora, y durante todo el documento, esta idea es básica para el
diseño que planteamos, puesto que en varios puntos el comportamiento del
sistema dependerá del tipo de ficheros que estemos tratando.

Dicho lo cual, pasamos a presentar el esquema del árbol de directorios y
ficheros como se puede apreciar en la figura:

\begin{code}
	/plfs
	|-slices
	| `-<slice>
	|   |-backup
	|   |-key
	|   |-passwd
	|   |-script
	|   |-nodes
	|   | `-<node>
	|   |   `-unshared
	|   `-shared
	`-nodes
	  `-<node>
	    `-slices
	      `-<slice> -> /plfs/slices/<slice>
\end{code}

A continuación una explicación de los diferentes ficheros y directorios:

\begin{description}
	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/backup}] :\\
		Este fichero contiene una URI que indica el repositorio central
		de backup donde se puedan encontrar los ficheros \texttt{shared}
		en su versión original (ver el apartado 2.x sobre Redespliegue).

	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/key}] :\\
		Este fichero contiene la clave privada ssh correspondiente al
		slice.

	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/passwd}] :\\
		Este fichero contiene el password asociado a la clave privada
		del slice.

	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/script}] :\\
		Éste fichero es un shell script que se ejecutará en cada máquina una
		vez hecho el desplegamiento.
		\\
		El script tiene un sólo parámetro que puede ser:

		% TODO(~): NOTA MENTAL: ¿al terminar la máquina virtual, ya
		% ejecutará dicho startup->stop?
		\begin{description}
			\item [start:] lleva a cabo las operaciones para
				arrancar el servicio desplegado

			\item [stop:] lleva a cabo las operaciones para parar
				el servicio

			\item [deploy:] lleva a cabo las operaciones necesarias
				para preparar el arranque del servicio
		\end{description}

	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/nodes/\textless{}node\textgreater/unshared}] :\\
		Éste directorio es propio de cada nodo en el contexto del slice que
		estamos mirando; puede servir para guardar ficheros de configuración
		específicos de cada máquina, sin hacer nunca un desplegamiento al
		grupo entero.

	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/shared}] :\\
		Éste directorio contiene los ficheros de los que se hace un
		desplegamiento al grupo entero (slice). La condición que deben
		cumplir los contenidos de éste directorio, es que sean
		inmutables de cara a \plfs.
\end{description}

Tal como está hecho el esquema, permitiría que alguien pusiera un directorio
de administración en, por ejemplo,
\texttt{/plfs/slices/\textless{}slice\textgreater/monitor} con estadísticas de
funcionamiento del slice, u otro
\texttt{/plfs/nodes/\textless{}node\textgreater/monitor} con estadísticas de
funcionamiento de un nodo concreto.



\section{Operaciones de \plfs}

Las posibles operaciones que \plfs permite, y por ende, que \kplfs implemente
y puede llegar a comunicar a \uplfs, son las propias de un sistema de
ficheros, es decir (según las operaciones que puede realizar un usuario desde
la línea de comandos):

\begin{description}
	\item [\texttt{ls}] :\\
		Operaciones de consulta de contenido de directorios y/o
		propiedades.

		\begin{itemize}
			\item Listado de slices (vía módulo \pldb)

			\item Listado de nodos (vía módulo \pldb)

			\item Listado de ficheros desplegados (\texttt{shared}
				y \texttt{unshared}).
		\end{itemize}

	\item [\texttt{mv}] :\\
		Lo que nosotros implementaremos será un \textit{move} de ficheros
		(aplicación a desplegar) a un slice (conjunto de nodos, representado
		por el directorio
		\texttt{/plfs/slices/\textless{slice\textgreater/shared}}) o nodo
		concreto (representado por el directorio
		\texttt{/plfs/slices/\textless{slice\textgreater/nodes/\textless{}node\textgreater/unshared}}).
		Es decir, \textbf{NO} permitiremos operaciones como:

		\begin{itemize}
			\item mover nodos

			\item mover slices

			\item \ldots
		\end{itemize}

	\item [\texttt{cp}] :\\
		Igual que en un \textit{move}, pero copiando los ficheros de orígen.

	\item [\textit{edición}] :\\
		La edición de un fichero provocará el redespliegue del mismo
		una vez sea cerrado.
		%TODO \cite (ver apartado 2.x sobre Redespliegue)

	\item [\textit{lectura}] :\\
		La lectura de un fichero provocará su importación previa en
		caso de no estar cacheado.

	\item [\texttt{rename}] :\\
		Soportado solo para ficheros y directorios que cuelgan de
		\texttt{shared} o \texttt{unshared}.

	\item [\texttt{rm}] :\\
		Soportado solo para ficheros y directorios que cuelgan de
		\texttt{shared} o \texttt{unshared}.

	\item [\texttt{mkdir}] :\\
		Según en que directorio se haga, las implicaciones son
		distintas:

		\begin{itemize}
			\item En \texttt{/plfs/slices}: Crea un slice sin nodos asociados

			\item En \texttt{/plfs/slices/\textless{}slice\textgreater/nodes}:
				Añade un nodo a un slice

			\item En \texttt{/plfs/nodes}: Añade un nodo al sistema
				PlanetLab

			\item En los directorios \texttt{shared} y
				\texttt{unshared} crea ése directorio, según
				corresponda
		\end{itemize}

		Los tres primeros puntos, como no entran dentro del
		desplegamiento de aplicaciones, no han sido pensados en
		profundidad para llevarlos a cabo.

	\item [\texttt{rmdir}] :\\
		Igual que en el caso anterior, según en que directorio se haga, las
		implicaciones son distintas:

		\begin{itemize}
			\item En \texttt{/plfs/slices}: Elimina un slice de PlanetLab

			\item En \texttt{/plfs/slices/\textless{}slice\textgreater/nodes}:
				Elimina un nodo de un slice

			\item En \texttt{/plfs/nodes}: Elimina un nodo del sistema
				PlanetLab

			\item En los directorios \texttt{shared} y
				\texttt{unshared} elimina el directorio que
				corresponda.
		\end{itemize}

		De nuevo, los tres primeros puntos, como no entran dentro del
		desplegamiento de aplicaciones, no han sido pensados en
		profundidad para llevarlos a cabo.

	\item [\texttt{touch}] :\\
		\textbf{NO} soportado.

	\item[\texttt{chown}, \texttt{chmod}] :\\
		\textbf{NO} soportado.

	\item[\texttt{ln}] :\\
		\textbf{NO} soportado.

	\item[Otras operaciones] :\\
		\textbf{NO} soportadas.
\end{description}



\section{Lógica de Desplegamiento}

Después de pensar diferentes posibilidades sobre como el usuario podría llevar
a cabo realmente el despliegue mediante el sistema de ficheros, hemos llegado
a la conclusión de que el usuario deberá realizar los siguientes pasos:

\begin{enumerate}
	\item 
		El usuario realiza \texttt{mv/cp} de los ficheros que
		desea desplegar, y estos son transmitidos mediante el proceso de
		despliegue que a continuación comentamos.

	\item 
		A continuación el usuario realiza \texttt{mv/cp} del script de
		arranque de despliegue, que también es desplegado mediante el
		mismo proceso hacia los nodos destino.
\end{enumerate}

Además, y como es obvio dada la naturaleza del propio sistema de ficheros, se
deberá cumplir la siguiente precondición antes de poder realizar el despliegue:

\textit{Precondición}: El slice destino está ya creado (ej: \texttt{mkdir}).

Una vez aclarado lo anterior, pasamos a describir como se realizaría de forma
general el despliegue de los ficheros a través de los diferentes componentes
del sistema:

\begin{enumerate}
	\item \kplfs $\rightarrow$ \uplfs \\
		Se informa de los ficheros de aplicación a desplegar i del slice
		o nodo destino (en el caso de ser un solo nodo el
		destino, la comunicació se establece directamente
		desde \uplfs hacia \dplc).

	\item \uplfs $\rightarrow$ \dpld \\
		\uplfs redirige el mensaje al componente indicado por este (\dpld),
		puesto que \kplfs puede manejar la vista de diferentes componentes.

	\item \dpld $\rightarrow$ \umcc \\
		Se comunica el slice destino, se dan los ficheros a desplegar y el
		shell script con los comandos a ejecutar una vez desplegados.

	\item \umcc $\rightarrow$ red \umcr \\
		Se inyectan los paquetes a enviar con destino a un grupo en la
		red \umc.\\
		En nuestro caso, el grupo destino sería un slice y el
		desplegamiento se realizará a aquellos nodos donde haya \dplc,
		de modo que serán siempre conocidos por la red multicast.

	\item intra-red \umcr (red multicast) \\
		Los nodos de la red \umcr encaminan el conjunto de paquetes
		hacia el grupo multicast de destino.\\
		En nuestro caso encaminarán los ficheros, script, etc de
		la aplicación a desplegar hacia los nodos de \dplc que forman
		parte del slice destino.

	\item red \umcr $\rightarrow$ \umcc \\
		Los paquetes llegan finalmente a los nodos que conforman el
		grupo destino multicast.
		En nuestro caso, los ficheros, script, etc. llegan finalmente a
		los nodos destino y son procesados por el módulo \umcc.

	\item \umcc $\rightarrow$ \dplc \\
		El módulo \umcc hace llegar los paquetes a desplegar al \dplc.

	\item \dplc $\rightarrow$ slice destino (ssh) \\
		Se despliega la aplicación (se copian los ficheros), y una vez hecho
		esto, se ejecuta el shell script adjuntado, en caso de estar
		presente. Para ello, primero ejecuta el \textbf{deploy} y luego
		el \textbf{start}.
		%En el apartado de sincronización veremos con más detalle las
		%diferentes posibilidades al realizar la ejecución del script
		%(sincronización).  <<-------- JA NO TE SENTIT
		%TODO: CITE => cite de que?!
\end{enumerate}

% TODO: NOTA:
% En caso de haber un fallo en cualquiera de los dos pasos anteriores,
% se devuelve un error al dpld origen (a modo de NACK) ??????
% Si todo es correcto anunciar al dpld??? Escalabilidad? ACK's



\section{Comunicación Multicast}

Como hemos comentado, el desplegamiento de los ficheros se hace sobre una red
multicast, por lo que hace falta describir algunas de sus características que
asumiremos de ahora en adelante para el diseño de los componentes del sistema.

Las operaciones que debe proporcionar esta red multicast son las siguientes:

\begin{description}
	\item[Join]:\\
		Inscripción a un grupo multicast.

	\item[Leave]:\\
		Desuscripción de un grupo multicast.
\end{description}

En nuestro caso, veremos como \dplc gestiona las inscripciones/desuscripciones
a los grupos multicast, dichos grupos serán realmente los slice destino a los
que se quiera realizar el desplegamiento de las aplicaciones. De modo que,
cuando se quiera hacer dicho desplegamiento, desde \dpld, no será necesario
consultar previamente a PlanetLab Central (mediante \pldb) sobre la pertinencia
de los nodos a un grupo. Es decir, el grupo ya está previamente creado.

El modo en que \dplc realiza las operaciones de inscripción/desuscripción y en
qué momento, lo veremos con más detalle en el capítulo \ref{TODO}.
%TODO CITE

Por otra parte el módulo \dpld también deberá suscribirse al grupo multicast
como oyente, para poder estar atento a los cambios que se produzcan en los
ficheros \texttt{shared} (en cuyo caso deberá realizar las operaciones que
veremos en el apartado 2.x de Redespliegue).

En consecuencia, \dpld deberá poder desuscribirse de un grupo, operación que
realizará o bien cuando se detenga el daemon o transcurrido cierto tiempo de
\textit{timeout}. %TODO aclarar este timeout.

Para hacer que la red multicast sea fiable nos hemos planteado las siguientes
posibilidades:

\begin{description}
	\item[NACK based]:\\
		Solamente se envían NACKS en caso de fallo en el envío. Es
		decir, no se realizan ACK's.

	\item[Tree ACK (TRACK)]:\\
		En este caso, se envían ACK's pero agrupados hacia la raíz del
		árbol multicast. Es decir, los ACK's de los "hijos" se agrupan
		en el "padre" para enviar un sólo ACK.
\end{description}

En nuestro caso, hemos pensado que sería positivo elegir la opción de TRACK,
puesto que ésta nos permitiría que \dpld realizara los tipos de
desplegamiento que presentamos en el apartado \ref{sect-redeployment}.
%TODO CITE



\section{Redespliegue}
\label{sect-redeployment}

El componente \fam se encarga de la monitorización de los ficheros etiquetados
como \texttt{shared}. De este modo, cuando se produce una modificación
incontrolada de estos ficheros, este componente avisa al \dplc para que a su
vez lo notifique a los nodos \dpld responsables de ese fichero, es decir,
aquellos que estén suscritos como oyentes de ese grupo multicast, o slice, del
cual son los responsables de los despliegues que se realicen en él.

De modo que entenderemos por modificación incontrolada, aquella que no provenga
de uno de estos nodos responsables.

Cuando los nodos \dpld responsables se percaten del aviso, procederán a
redesplegar el fichero siguiendo los pasos a continuación:

\begin{enumerate}
	\item \dplc realiza un script->stop deteniendo momentáneamente la
		aplicación en ese nodo corrupto.

	\item se despliegan los ficheros desde un nodo del mismo slice (un nodo
		hermano hablando desde el punto de vista de árboles de
		directorios) que no sea corrupto.

	\item \dplc realiza un script->start para reiniciar la aplicación.
\end{enumerate}

Obviamente surge un posible problema como consecuencia de este diseño, y es que
podría suceder que por circunstancias no existiera ningún otro nodo no corrupto
(o bien porque todos los hermanos lo están, o bien porque es el único nodo
activo del slice).

De modo que la solución que proponemos es la de mantener en un repositorio
central los ficheros originales desplegados como \texttt{shared}.
La URI de este repositorio es la que se indica en el fichero \texttt{backup}
que hemos presentado en el esquema de \plfs.

Finalmente, si la recuperación mediante este repositorio no pudiera llevarse a
cabo, se invalidaría el nodo del slice, y quedaría como corrupto.



% TODO(~) (Apartats3.2, 3.6 i 4.3):
%
%	Añadir un nodo al slice una vez ya desplegada la
%aplicacion
%
%	Solucio: DPLC d'aquest node, ha de comunicar a DPLD que
%	s'hi afegeix al qual se li ha de fer desplegament!! (3.2, 3.6)
%		=> PROTOCOL DE WARNINGS AMB AUTENTIFICACIO!!! (4.1)
%


% TODO (!!!!!): Cuando se abren las conexiones SSH? timeout?
%       Cuando se dan las claves y passwd ssh?
%


% TODO(?): 'ls' => renew => cheksums??
%
% TODO(~)  'ls' de unshared ??
% 	=> MIRAR NODO CONCRETO => DPLC TIENE QUE AGREGAR FUNCIONALIDAD 	(3.6)
%
% TODO(~)  'ls' de shared ??
%
%	Solucio Possible:
%		1) dpld escull a l'atzar un node del slice a qui preguntar
%		<shared> (essent aquest no corrupte).			(3.2)
%		2) si dpld s'assabenta que un node es corrupte, envia l'ordre
%		de fer stop.
%			a) si no es sincrhonized, el redesplega amb
%			deployAppToNode
%			b) si es synchronized,
%				- Redesplegar tot per ex. si hi ha un minim de
%				nodes no corruptes i en funcionament.
%
%
%	=> FAM ha de comunicar a DPLC si hi ha canvis als fitxers
%		=> FAM ha de saber quins fitxers son shared		(3.7)
%		=> DPLC ha d'arrancar FAM!!!				(3.6)
%
%	A MES SI FAM SAP SI SON SHARED O UNSHARED, permet que DPLC li soliciti
%	els fitxers de cada tipus segons la peticio que faci DPLD.

