\chapter{Arquitectura del sistema}

\textbf{\textit{En este capítulo comentamos la arquitectura general del
sistema, los componentes necesarios, el sistema de ficheros y las operaciones
que debe implementar.}}



\section{Componentes}
\label{sect:components}

En este apartado mostraremos un esquema inicial de los principales
componentes que necesitaría el sistema presentado (figura
\ref{fig:arquitectura}):

\begin{description}
	\item [\kplfs:] PlanetLab File System (Kernel-mode Side)\\
		Componente del sistema de ficheros para PlanetLab que funciona dentro
		del núcleo de Linux.
		\\
		Este componente, puede en realidad ser varios, cada uno dando
		unos servicios concretos (uno para el despliegue, uno para
		monitorización, \ldots).

	\item [\uplfs:] PlanetLab File System (User-mode Side)\\
		Componente del sistema de ficheros para PlanetLab que funciona
		en modo usuario.
		\\
		Este componente en realidad hace de intermediario entre el
		propio sistema de ficheros (es decir, el núcleo) y los
		componentes que permiten realizar las funcionalidades y
		comunicaciones (en nuestro caso, sería \dpld).

	% TODO: intercambiar dpld <--> dplc ?!?! tendría mas sentido?
	\item [\dpld:] Deployer Daemon\\
		Componente encargado de mandar las órdenes de despliegue y
		mantener actualizada la visión que tiene el usuario sobre los
		slices de despliegue.

	\item [\pldb :] PlanetLab Data Base (para obtener inicialmente la lista
		de nodos de un slice)\\
		Componente para obtener datos de PlanetLab Central (PLC).

	\item [\umcc :] User-mode Multicast Client\\
		Componente encargado de las comunicaciones entre un sistema
		cualquiera y la red multicast.

	\item [\umcr :] User-mode Multicast Router\\
		Componente encargado de enrutar paquetes multicast.

	\item [\dplc :] Deployer Client\\
		Componente encargado de desplegar los ficheros en un nodo e
		informar de los slices que tiene creados.

	\item [\famon:] File Alteration Monitor\\
		Componente encargado de vigilar qué ficheros cambian en un nodo.
\end{description}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{arquitectura.eps}
	\caption{Arquitectura del Sistema}
	\label{fig:arquitectura}
\end{figure}



\section{Esquema de \plfs (PlanetLab File System)}
\label{sect:plfsschema}

Antes de comentar el esquema del árbol de ficheros y directorios de \plfs, es
necesario dejar bien claro que hemos diseñado el sistema con la idea de
permitir una cierta flexibilidad al usuario y que pueda diferenciar entre dos
tipos de ficheros:

\begin{description}
	\item [Los ficheros que se despliegan como shared]:\\
		Que todos los nodos del slice destino tendrán por igual, bajo la
		condición que además deben de ser inmutables (ej: el programa
		principal de la aplicación).

	\item [Los ficheros que se despliegan como unshared]:\\
		Que serán propios de cada nodo (ej: cada nodo podría asumir un
		rol distinto si tuviera ficheros de configuración propios).
\end{description}

Como veremos ahora, y durante todo el documento, esta idea es básica para el
diseño que planteamos, puesto que en varios puntos el comportamiento del
sistema dependerá del tipo de ficheros que estemos tratando.

Dicho lo cual, pasamos a presentar el esquema del árbol de directorios y
ficheros como se puede apreciar en la figura:

\newpage

\begin{code}
	/plfs
	|-publickey
	|-privatekey
	|-slices
	| `-<slice>
	|   |-backup
	|   |-key
	|   |-passwd
	|   |-status
	|   |-script
	|   |-nodes
	|   | `-<node>
	|   |   |-version
	|   |   `-unshared
	|   `-shared
	`-nodes
	  `-<node>
	    `-slices
	      `-<slice> -> /plfs/slices/<slice>
\end{code}

A continuación una explicación de los diferentes ficheros y directorios:

\begin{description}
	\item[\texttt{/plfs/publickey}] :\\
		Este fichero contiene la clave pública del \dpld para que
		funcione todo el sistema de seguridad (ver apartado
		\ref{sect:segurity}).

	\item[\texttt{/plfs/privatekey}] :\\
		Este fichero contiene la clave privada del \dpld para que
		funcione todo el sistema de seguridad (ver apartado
		\ref{sect:segurity}).

	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/backup}] :\\
		Este fichero contiene una URI que indica el repositorio central
		de backup donde se puedan encontrar los ficheros \texttt{shared}
		en su versión original (ver el apartado \ref{sect:redeployment}
		sobre Redespliegue).

	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/key}] :\\
		Este fichero contiene la clave privada SSH correspondiente al
		slice.

	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/passwd}] :\\
		Este fichero contiene el password asociado a la clave privada
		del slice.

	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/status}] :\\
		Este fichero permite conocer el estado de despliegue de los
		nodos del slice.

	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/script}] :\\
		Este fichero es un shell script que se ejecutará en cada máquina
		una vez hecho el despliegue.
		\\
		El script tiene un sólo parámetro que puede ser:

		\begin{description}
			\item [start:] lleva a cabo las operaciones para
				arrancar el servicio desplegado

			\item [stop:] lleva a cabo las operaciones para parar
				el servicio (NOTA: al terminar una máquina
				virtual, PlanetLab ejecuta el script con este
				parámetro)

			\item [deploy:] lleva a cabo las operaciones necesarias
				para preparar el arranque del servicio
				(instalación de la aplicación y logueo de los
				ficheros generados durante ésta)

			\item [clean:] elimina los ficheros que ha desplegado
				el \texttt{script$\rightarrow$deploy}
		\end{description}

	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/nodes/\textless{}
		node\textgreater/version}] :\\
		Este fichero contiene el número de version del último
		despliegue (ver apartado \ref{sect:versioning}).

	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/nodes/\textless{}
		node\textgreater/unshared}] :\\
		Éste directorio es propio de cada nodo en el contexto del slice
		que estamos mirando; puede servir para guardar ficheros de
		configuración específicos de cada máquina, sin hacer nunca un
		despliegue al grupo entero.

	\item[\texttt{/plfs/slices/\textless{}slice\textgreater/shared}] :\\
		Éste directorio contiene los ficheros de los que se hace un
		despliegue al grupo entero (slice). La condición que deben
		cumplir los contenidos de éste directorio, es que sean
		inmutables de cara a \plfs.
\end{description}

Tal como está hecho el esquema, permitiría que alguien pusiera un directorio
de administración en, por ejemplo,
\texttt{/plfs/slices/\textless{}slice\textgreater/monitor} con estadísticas de
funcionamiento del slice, u otro
\texttt{/plfs/nodes/\textless{}node\textgreater/monitor} con estadísticas de
funcionamiento de un nodo concreto.



\section{Operaciones de \plfs}
\label{sect:plfsoperations}

Las posibles operaciones que \plfs permite, y por ende, que \kplfs implemente
y puede llegar a comunicar a \uplfs, son las propias de un sistema de
ficheros, es decir (según las operaciones que puede realizar un usuario desde
la línea de comandos):

\begin{description}
	\item [\texttt{ls}] :\\
		Operaciones de consulta de contenido de directorios y/o
		propiedades.

		\begin{itemize}
			\item Listado de slices (vía módulo \pldb)

			\item Listado de nodos (vía módulo \pldb)

			\item Listado de ficheros desplegados (\texttt{shared}
				y \texttt{unshared}).
		\end{itemize}

	\item [\texttt{mv}] :\\
		Lo que nosotros implementaremos será un \textit{move} de
		ficheros (aplicación a desplegar) a un slice (conjunto de nodos,
		representado por el directorio
		\texttt{/plfs/slices/\textless{slice\textgreater/shared}}) o
		nodo concreto (representado por el directorio
	\texttt{/plfs/slices/\textless{slice\textgreater/nodes/\textless{}
	node\textgreater/unshared}}).
		Es decir, \textbf{NO} permitiremos operaciones como:

		\begin{itemize}
			\item mover nodos

			\item mover slices

			\item \ldots
		\end{itemize}

	\item [\texttt{cp}] :\\
		Igual que en un \textit{move}, pero copiando los ficheros de
		orígen.

	\item [\textit{edición}] :\\
		La edición de un fichero provocará el redespliegue del mismo
		una vez sea cerrado (ver apartado \label{sect:redeployment}).

	\item [\textit{lectura}] :\\
		La lectura de un fichero provocará su importación previa en
		caso de no estar cacheado.

	\item [\texttt{rename}] :\\
		Soportado solo para ficheros y directorios que cuelgan de
		\texttt{shared} o \texttt{unshared}.

	\item [\texttt{rm}] :\\
		Soportado solo para ficheros y directorios que cuelgan de
		\texttt{shared} o \texttt{unshared}.

	\item [\texttt{mkdir}] :\\
		Según en que directorio se haga, las implicaciones son
		distintas:

		\begin{itemize}
			\item En \texttt{/plfs/slices}: Crea un slice sin nodos
				asociados

			\item En
		\texttt{/plfs/slices/\textless{}slice\textgreater/nodes}:
				Añade un nodo a un slice

			\item En \texttt{/plfs/nodes}: Añade un nodo al sistema
				PlanetLab

			\item En los directorios \texttt{shared} y
				\texttt{unshared} crea ése directorio, según
				corresponda
		\end{itemize}

		Los tres primeros puntos, como no entran dentro del
		despliegue de aplicaciones, no han sido pensados en
		profundidad para llevarlos a cabo.

	\item [\texttt{rmdir}] :\\
		Igual que en el caso anterior, según en que directorio se haga,
		las implicaciones son distintas:

		\begin{itemize}
			\item En \texttt{/plfs/slices}: Elimina un slice de
				PlanetLab

			\item En
		\texttt{/plfs/slices/\textless{}slice\textgreater/nodes}:
				Elimina un nodo de un slice

			\item En \texttt{/plfs/nodes}: Elimina un nodo del
				sistema PlanetLab

			\item En los directorios \texttt{shared} y
				\texttt{unshared} elimina el directorio que
				corresponda.
		\end{itemize}

		De nuevo, los tres primeros puntos, como no entran dentro del
		despliegue de aplicaciones, no han sido pensados en
		profundidad para llevarlos a cabo.

	\item [\texttt{touch}] :\\
		\textbf{NO} soportado.

	\item[\texttt{chown}, \texttt{chmod}] :\\
		\textbf{NO} soportado.

	\item[\texttt{ln}] :\\
		\textbf{NO} soportado.

	\item[Otras operaciones] :\\
		\textbf{NO} soportadas.
\end{description}



\section{Descubrimiento de entidades del sistema}
\label{sect:discovery}

El sistema, además de las funcionalidades propias de despliegue de
aplicaciones, debe poder realizar las siguientes operaciones para poder saber
como llegar y a dónde puede llegar. Es decir, debe poder conseguir la
información necesaria sobre los slices disponibles en PlanetLab y sus nodos, y
por otra parte, debe poder descubrir su punto de entrada a la red multicast
desde el exterior.

De modo que el sistema debe permitir:

\begin{itemize}
	\item Descubrimiento de slices en PlanetLab \\
		Al acceder al sistema de ficheros (\kplfs-\uplfs), hay que
		preguntar a un servicio de PlanetLab si existe o no un slice
		dado (al hacer un move de un ejecutable a un directorio que es
		un slice) o cuáles existen (al hacer un ls para ver los slices
		disponibles).

		Para ello, hace falta encontrar el nodo con el servicio de
		PlanetLab de BD \textbf{más cercano} (por ahora solo hay uno, y
		es \textit{PlanetLab Central}) y preguntarle:

		\begin{itemize}
			\item si existe un slice concreto

			\item qué slices hay
		\end{itemize}

	\item Descubrimiento de nodos en PlanetLab \\
		Como en el caso anterior, nos hace falta preguntar a PlanetLab
		por los nodos, tanto a nivel general, como por los asociados a
		un slice.

	\item Descubrimiento del nodo de entrada a la red de distribución
		multicast.
\end{itemize}

Con tal de minimizar la latencia y ocupación innecesaria del ancho de banda,
para todos los anteriores puntos hace falta encontrar el nodo \textbf{más
cercano} que da un servicio concreto.

Para ello, se podría utilizar un servicio DNS al estilo Akamai, tal como
utiliza google, o algo como CoDNS \cite{CoDNS}, subproyecto de Codeen
\cite{Codeen}, o PowerDNS \cite{PowerDNS} con su backend geográfico, que nos dan
la IP del nodo más cercano que proporciona un servicio conreto. Cabe notar,
además, que éste es un servicio que PlanetLab planea dar a todos sus nodos y que
tiene como tarea pendiente implementar.

Como el servicio de DNS se utiliza siempre de forma transparente, aunque dé un
servicio no habitual de localidad, no lo pondremos en el esquema de nuestros
componentes para mejorar su claridad.



\section{Lógica de Despliegue}
\label{sect:deployment}

\nocite{Bamboo}

Después de pensar diferentes posibilidades sobre como el usuario podría llevar
a cabo realmente el despliegue mediante el sistema de ficheros, y mirar
otros proyectos dedicados al despliegue de aplicaciones \cite{PLSDT, RSTB,
Stork}, u otras aplicaciones distribuidas para PlanetLab \cite{MON, NWS}, hemos
llegado a la conclusión de que el usuario deberá realizar los siguientes pasos:

\begin{enumerate}
	\item El usuario realiza \texttt{mv/cp} de los ficheros que
		desea desplegar, y estos son transmitidos mediante el proceso de
		despliegue que a continuación comentamos.

	\item A continuación el usuario realiza \texttt{mv/cp/touch} del script
		de arranque de despliegue, que también es desplegado mediante el
		mismo proceso hacia los nodos destino.
\end{enumerate}

Además, y como es obvio dada la naturaleza del propio sistema de ficheros, se
deberá cumplir la siguiente precondición antes de poder realizar el
despliegue:
\\

\textit{Precondición}: El slice destino está ya creado (ej: \texttt{mkdir}).\\

Una vez aclarado lo anterior, pasamos a describir como se realizaría de forma
general el despliegue de los ficheros a través de los diferentes componentes
del sistema:

\begin{enumerate}
	\item \kplfs $\rightarrow$ \uplfs \\
		Se informa de los ficheros de aplicación a desplegar i del slice
		o nodo destino (en el caso de ser un solo nodo el
		destino, la comunicació se establece directamente
		desde \uplfs hacia \dplc).

	\item \uplfs $\rightarrow$ \dpld \\
		\uplfs redirige el mensaje al componente indicado por este
		(\dpld), puesto que \kplfs puede manejar la vista de diferentes
		componentes.

	\item \dpld $\rightarrow$ \umcc \\
		Se comunica el slice destino, se dan los ficheros a desplegar y
		el shell script con los comandos a ejecutar una vez
		desplegados, indicando la versión que se está desplegando.

	\item \umcc $\rightarrow$ red \umcr \\
		Se inyectan los paquetes a enviar con destino a un grupo en la
		red \umc.\\
		En nuestro caso, el grupo destino sería un slice y el
		despliegue se realizará a aquellos nodos donde haya \dplc,
		de modo que serán siempre conocidos por la red multicast.

	\item intra-red \umcr (red multicast) \\
		Los nodos de la red \umcr encaminan el conjunto de paquetes
		hacia el grupo multicast de destino.\\
		En nuestro caso encaminarán los ficheros, script, etc de
		la aplicación a desplegar hacia los nodos de \dplc que forman
		parte del slice destino.

	\item red \umcr $\rightarrow$ \umcc \\
		Los paquetes llegan finalmente a los nodos que conforman el
		grupo destino multicast.
		En nuestro caso, los ficheros, script, etc. llegan finalmente a
		los nodos destino y son procesados por el módulo \umcc.

	\item \umcc $\rightarrow$ \dplc \\
		El módulo \umcc hace llegar los paquetes a desplegar al \dplc.

	\item \dplc $\rightarrow$ slice destino (SSH) \\
		Se despliega la aplicación (se copian los ficheros), y una vez
		hecho esto, se ejecuta el shell script (que es el último
		fichero en llegar). Para ello, primero se desinstala la
		anterior aplicación (\texttt{script$\rightarrow$clean}), se
		copian los nuevos ficheros, y luego se despliega la aplicación
		(\texttt{script$\rightarrow$deploy}) y se pone en marcha
		(\texttt{script$\rightarrow$start}).
\end{enumerate}

Como se ve en el apartado \ref{sect:redeployment}, no es necesario controlar
los casos en que algún despliegue no se lleve a cabo en algún nodo, ya que
entre ellos mismos se encargarán de mantener la coherencia del despliegue.

El fichero \texttt{/plfs/slices/\textless{}slice\textgreater/status} contiene,
para cada nodo del slice, si se ha podido contactar con él y, en caso
afirmativo, en qué estado de despliegue se encuentra el nodo.

Esta información se deriva a través del contenido del acceso a los ficheros
\texttt{/plfs/slices/\textless{}slice\textgreater/nodes/\textless{}
node\textgreater/version}, dejando comprobar en qué estado se encuentra el
despliegue al comparar la versión de los nodos con la actual.



\section{Comunicación Multicast}
\label{sect:multicast}

Como hemos comentado, el despliegue de los ficheros se hace sobre una red
multicast, por lo que hace falta describir algunas de sus características que
asumiremos de ahora en adelante para el diseño de los componentes del sistema.

Las operaciones que debe proporcionar esta red multicast son las siguientes:

\begin{description}
	\item[Join]:\\
		Inscripción a un grupo multicast.

	\item[Leave]:\\
		Desuscripción de un grupo multicast.
\end{description}

En nuestro caso, veremos como \dplc gestiona las inscripciones/desuscripciones
a los grupos multicast, dichos grupos serán realmente los slice destino a los
que se quiera realizar el despliegue de las aplicaciones. De modo que,
cuando se quiera hacer dicho despliegue, desde \dpld, no será necesario
consultar previamente a PlanetLab Central (mediante \pldb) sobre la pertinencia
de los nodos a un grupo. Es decir, el grupo ya está previamente creado.

El modo en que \dplc realiza las operaciones de inscripción/desuscripción y en
qué momento, lo veremos con más detalle en el apartado \ref{sect:umcc}.

Por otra parte el módulo \dpld también deberá suscribirse al grupo multicast
como oyente, para poder estar atento a los cambios que se produzcan en los
ficheros \texttt{shared} (en cuyo caso deberá realizar las operaciones que
veremos en el apartado \ref{sect:redeployment}).

En consecuencia, \dpld deberá poder desuscribirse de un grupo, operación que
realizará o bien cuando se detenga el daemon o transcurrido cierto tiempo de
\textit{timeout} por inactividad.

Para hacer que la red multicast sea fiable nos hemos planteado las siguientes
posibilidades:

\begin{description}
	\item[NACK based]:\\
		Solamente se envían NACKS en caso de fallo en el envío. Es
		decir, no se realizan ACK's.

	\item[Tree ACK (TRACK) \cite{rfc-track}]:\\
		Los receptores envían periódicamente informes a el nodo padre
		sobre lo que han recibido y lo que no (paquetes perdidos).\\
		Cada padre agrega los informes de sus hijos en un TRACK y los
		pasa a su padre.\\
		Se repite el proceso hasta el nodo del que proceden los
		paquetes, el cual puede realizar las operaciones
		correspondientes según el TRACK total obtenido.
\end{description}

Creemos que TRACK es más completo y permitiría realizar extensiones al sistema,
como por ejemplo a modo de monitorización o posibilidad de extender el modelo
de despliegue a, por ejemplo, exigir un mínimo de N nodos.



\section{Versionamiento de despliegues}
\label{sect:versioning}

Como los nodos de los slices pueden recibir varios despliegues en el tiempo y de
manera secuencial, y queremos tener los nodos de un mismo slice sincronizados
al último despliegue realizado, necesitamos una manera de controlar las
versiones que nos permita detectar las situaciones que comentamos en el
apartado \ref{sect:redeployment}.

Como presuponemos que los despliegues son relativamente poco frecuentes, y no
soportamos la posibilidad de realizar despliegues simultáneos (en parte por que
no tienen sentido por la naturaleza del problema), consideramos que la fecha y
la hora en un formato común entre todos los nodos (ej:UTC o GMT), son
identificadores suficientes de la versión de un despliegue, presuponiendo que
los nodos de PlanetLab están sincronizados a esta hora con unos límites de
error aceptables.

Este número de versión es accesible a través del fichero
\texttt{/plfs/slices/\textless{}slice\textgreater/nodes/\textless{}
node\textgreater/version}.

Cabe destacar un tipo de versión especial que llamaremos ``versión corrupta''
que será considerada como la más vieja de todas las versiones.



\section{Redespliegue}
\label{sect:redeployment}

Existen dos casos en los que es necesario un redespliegue:

\begin{description}
	\item[Caída de nodos] : \\
		Los nodos en PlanetLab "caen" de forma aleatoria, o bien un nodo
		concreto (cuelgue de la máquina, reinicio, pérdida
		de conectividad, etc.) o por grupos institucionales
		(fallo eléctrico de una zona, caída del acceso a la red, etc.).
		Estas situaciones se detectan en caso de rearrancar \dplc (por
		una caída de la máquina) o bien cuando el nodo queda reconectado
		a la red multicast (después de haber sido expulsado por
		falta de respuesta). \\
		Será necesario realizar un redespliegue, si al volver al
		grupo, el nodo detecta que hay una versión más nueva de
		despliegue.

	\item[Modificación de ficheros] : \\
		Cuando se produce una modificación incontrolada (que no viene
		de uno de los \dpld registrados) de los ficheros de un nodo al
		que se le ha realizado un despliegue, y estos son de tipo
		\textit{shared}, \famon se encarga de detectarlo y avisar a
		\dplc, que se encargará de avisar a los \dpld
		registrados para que hagan una invalidación de
		dichos ficheros.
		\\
		En este caso es necesario además marcar la versión como
		``versión corrupta'' (nunca será rearrancada una
		aplicación marcada de esta manera).
\end{description}

Cuando se detectan uno de estos dos casos, antes de nada, se detiene la
aplicación mediante el script de despliegue (script$\rightarrow$stop) y
posteriormente se realiza el proceso de redespliegue.

Para realizar el redespliegue es necesario que los \dplc mantengan una lista de
\textit{peers} para cada slice al que pertenece. Cada lista contiene N
nodos con el último número de versión conocido para cada uno de ellos (a modo de
\textit{vector clock}).

Para obtener esta lista, cuando \dplc arranca obtiene las direcciones de N nodos
cualesquiera mediante \pldb y a continuación les pregunta a cada uno de ellos
su número de versión dando a cambio el suyo propio.

En caso de que un nodo no responda se sustituye por otro que no
esté en la lista de forma aleatoria. Análogamente, en el caso de que esté en la
lista pero posteriormente se pierda la conexión con él (ej: detección por pings
periódicamente con períodos largos para no saturar la red), también será
sustituido.

En el caso de que sea el propio nodo el que haya perdido la conexión, \dplc
quedará suspendido hasta reactivar la conexión, momento en el que intentará
revalidar las entradas de su tabla de \textit{peers}.

El proceso de redespliegue (cuando sea necesario tal como hemos comentado
antes) se realizará mediante los siguientes pasos:

\begin{enumerate}
	\item En caso de no tener los datos de SSH, \dplc se los pedirá a algún
		nodo de la lista para poder abrir las conexiones.

	\item Una vez abiertas las conexiones, se pide un despliegue unicast a
		uno de los \textit{peers} de la lista.

	\item Se procede al despliegue de los ficheros actualizando así la
		versión.
\end{enumerate}

Obviamente surge un posible problema como consecuencia de este diseño, y es que
podría suceder que por circunstancias no existiera ningún nodo con una versión
(o bien porque todos los hermanos lo están, o bien porque es el único nodo
activo del slice).

De modo que la solución que proponemos es la de mantener en un repositorio
central los ficheros originales desplegados como \texttt{shared}, de modo que
si \dplc no puede contactar con ningún nodo no corrupto pueda descargar estos
ficheros.

La URI de este repositorio es la que se indica en el fichero \texttt{backup}
que hemos presentado en el esquema de \plfs.

Para evitar un colapso de la red los cambios de versión no se informan de
immediato al producirse dicho cambio (excepto del cambio de versión a ``versión
corrupta'' que produce un aviso instantáneo), sino que se utilza un mecanismo
de coherencia relajada a través de los mensajes que se envían los
\textit{peers} para actualizar sus listas de versiones, aprovechando la
entropía que propicia el hecho de que cada uno envía dichos mensajes en tiempos
muy probablemente diferentes (porque han empezado en un instante distinto).

Como mejora, sería interesante detectar qué nodos están en una misma subred, es
decir, que forman parte de una misma institución y comparten una red mucho más
rápida que Internet (ya sea por configuración manual o por detección a través
de las interfícies configuradas), para así delegar un posible proceso de
\textit{redeployment} de todo el grupo a un solo nodo, que luego haría el mismo
\textit{redeployment} al resto de nodos del grupo (hasta pudiendo así
aprovecharse de las capacidades de multicast del nivel de enlace).



\section{Seguridad}
\label{sect:security}

Puesto que la autentificación de todos los clientes es obligatoria en
PlanetLab, cada vez que un cliente (\dpld en nuestro caso) intente una operación
con un nodo (\dplc) con el que no está autentificado, deberá primero realizar
el proceso de autentificación.

Dicho proceso va dirigido a proporcionar la clave y password SSH a los \dplc y
verificar que pueden realizar la conexión (que no cerrarán hasta
pasado un cierto tiempo de inactividad, para evitar la costosa operación de
realizar la conexión SSH), verificando así la autenticidad del acceso del
cliente a un slice concreto.

Para ello, cada \dpld tiene su par de llaves, que llamaremos $K_{pub_{d}}$ y
$K_{priv_{d}}$ para las claves pública y privada, respectivamente, y los \dplc,
que son administrados por una única identidad de confianza, tienen todos un
mismo par de llaves, que llamaremos $K_{pub_{c}}$ y $K_{priv_{c}}$.

Para controlar que los datos recibidos en los \dplc proceden de un \dpld
previamente autentificado, utilizaremos una firma electrónica con
$K_{priv_{d}}$ (puesto que los \dplc conocen $K_{pub_{d}}$).

Los pasos a seguir son:

\begin{enumerate}
	\item Se consulta con \pldb y se escoge un nodo cualquiera
		del slice con el que nos queremos comunicar.
		\label{step:auth_get_node}

	\item Se pide la clave pública del nodo seleccionado, $K_{pub_{c}}$, y
		se le da la pública del \dpld, $K_{pub_{d}}$.\\
		Si el nodo no responde, se vuelve al paso
		\ref{step:auth_get_node}).

	\item Se hace una petición de registro unicast al nodo, con la clave y
		password SSH encriptados con $K_{pub_{c}}$, juntamente con
		$K_{pub_{d}}$ para el envío encriptado de datos sensibles
		(listado de ficheros, etc.).\\
		Si la operación falla, se aborta el proceso de registro.
		Si el nodo no responde, se vuelve al paso
		\ref{step:auth_get_node}).

	\item Se hace una petición de registro multicast al grupo, con la clave
		y password SSH encriptados con $K_{pub_{c}}$, juntamente con
		$K_{pub_{d}}$.
\end{enumerate}

Hay varios casos en los que una operación realizada por un \dpld no podría ser
llevada a cabo en un \dplc:

\begin{itemize}
	\item Se añade un nuevo nodo después de haber hecho el proceso de
		registro, de modo que \dpld no está registrado en ese nodo.

	\item \dplc cierra la conexión SSH por falta de operaciones durante
		cierto tiempo de \textit{timeout}, de modo que no se puede
		hacer llegar las operaciones enviadas por \dpld.

	\item Cambio del par de claves del \dplc, de forma que no
		se pueden verificar las operaciones enviadas por \dpld.
\end{itemize}

La solución que hemos pensado para estos tres casos anteriores, es la de
realizar nuevos registros. En el primer caso, \dplc avisaría mediante una
operación (ver apartado \ref{sect:dplc}), y \dpld realizaría un
registro unicast en ese nodo. En los otros casos \dpld realizaría de
nuevo el registro multicast cada cierto tiempo (política de revalidación).

Cabe notar que las conexiones que \dpld mantiene con los slices tienen un
\textit{timeout} asociado, de modo que una vez expirado se perderán dichas
conexiones y si posteriormente se quisiera realizar una operación en
alguno de esos slices se deberá reiniciar de cero el proceso de registro
(momento en el que se intentará revalidar los ficheros que tuviera cacheados el
\dpld).

Ahora bien, \dplc no pierde los datos de conexión (clave y password SSH),
puesto que como se explica en el apartado \ref{sect:redeployment}, estos son
necesarios para mantener la coherencia del slice.

En el caso en que se hayan cambiado la clave y/o el password SSH y que la
conexión SSH esté cerrada, el \dpld recibiría algunas operaciones de aviso (ver
apartado \ref{sect:dpld}) procedentes de los \dplc destinatarios con la conexión
SSH cerrada (que serán una minoría debido a la política de revalidación de
registros comentada anteriormente).

Como los datos desplegados por \dpld van firmados para evitar problemas de
suplantación de identidad, en el caso de que \dpld modifique su par de claves
deberá propagar la clave pública mediante un registro multicast a los slices
con los que tuviera una relación abierta.
